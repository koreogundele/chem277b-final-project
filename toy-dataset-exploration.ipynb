{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path:    /gdb11_s01/gdb11_s01-0\n",
      "  Smiles:       [H]C([H])([H])[H]\n",
      "  Symbols:      ['C', 'H', 'H', 'H', 'H']\n",
      "  Coordinates:  (5400, 5, 3)\n",
      "  Energies:     (5400,) \n",
      "\n",
      "Path:    /gdb11_s01/gdb11_s01-1\n",
      "  Smiles:       [H]N([H])[H]\n",
      "  Symbols:      ['N', 'H', 'H', 'H']\n",
      "  Coordinates:  (3600, 4, 3)\n",
      "  Energies:     (3600,) \n",
      "\n",
      "Path:    /gdb11_s01/gdb11_s01-2\n",
      "  Smiles:       [H]O[H]\n",
      "  Symbols:      ['O', 'H', 'H']\n",
      "  Coordinates:  (1800, 3, 3)\n",
      "  Energies:     (1800,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyanitools import anidataloader\n",
    "\n",
    "# fet the HDF5 file containing the data\n",
    "hdf5file = 'ani_gdb_s01.h5'\n",
    "\n",
    "# construct the data loader class\n",
    "adl = anidataloader(hdf5file)\n",
    "\n",
    "# print the species of the data set one by one\n",
    "for data in adl:\n",
    "#     print(data.keys())\n",
    "\n",
    "    # extract the data\n",
    "    P = data['path']\n",
    "    X = data['coordinates']\n",
    "    E = data['energies']\n",
    "    S = data['species']\n",
    "    sm = data['smiles']\n",
    "\n",
    "    # print the data\n",
    "    print(\"Path:   \", P)\n",
    "    print(\"  Smiles:      \",\"\".join(sm))\n",
    "    print(\"  Symbols:     \", S)\n",
    "    print(\"  Coordinates: \", X.shape)\n",
    "    print(\"  Energies:    \", E.shape, \"\\n\")\n",
    "\n",
    "# closes the H5 data file\n",
    "adl.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bard AI says:\n",
    "To encode molecular strings for a ANN, I would use the following steps:\n",
    "\n",
    "1. Tokenize the molecular strings: The first step is to tokenize the molecular strings into individual tokens. This can be done using a variety of techniques, such as regular expressions or a pre-trained text tokenizer.\n",
    "2. Embed the tokens: The next step is to embed the tokens into a numerical representation. This can be done using a variety of techniques, such as one-hot encoding, word2vec, or GloVe.\n",
    "3. Pad or truncate the sequences: The final step is to pad or truncate the sequences to a fixed length. This is necessary because ANNs typically require input data to be of a fixed length.\n",
    "\n",
    "Here is an example of how to encode molecular strings using the above steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# function to tokenize molecular strings\n",
    "def tokenize_smiles(smiles):\n",
    "  # Split the SMILES string into individual tokens\n",
    "  tokens = re.split(r'[^A-Z0-9]', smiles)\n",
    "\n",
    "  # Remove empty tokens\n",
    "  tokens = [token for token in tokens if token]\n",
    "\n",
    "  return tokens\n",
    "\n",
    "# Define a function to embed molecular strings\n",
    "def embed_smiles(smiles, embedding_dim):\n",
    "  # Tokenize the SMILES string\n",
    "  tokens = tokenize_smiles(smiles)\n",
    "\n",
    "  # Create a one-hot encoding of each token\n",
    "  embeddings = np.zeros((len(tokens), embedding_dim))\n",
    "  for i in range(len(tokens)):\n",
    "    embeddings[i, tokens[i]] = 1\n",
    "\n",
    "  return embeddings\n",
    "\n",
    "# Define a function to pad or truncate sequences\n",
    "def pad_or_truncate_sequences(sequences, max_len):\n",
    "  # Pad the sequences with zeros\n",
    "  sequences = np.pad(sequences, ((0, 0), (0, max_len - sequences.shape[1])), 'constant')\n",
    "\n",
    "  # Truncate the sequences to the maximum length\n",
    "  sequences = sequences[:, :max_len]\n",
    "\n",
    "  return sequences\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Define the embedding dimension\n",
    "embedding_dim = 128\n",
    "\n",
    "# Encode a SMILES string\n",
    "smiles = 'CCC'\n",
    "embedding = embed_smiles(smiles, embedding_dim)\n",
    "\n",
    "# Pad or truncate the sequence to a fixed length\n",
    "max_len = 1024\n",
    "padded_embedding = pad_or_truncate_sequences(embedding, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT also suggests:\n",
    "Embedding Layers:\n",
    "- Instead of one-hot encoding, you can use embedding layers. These layers are often part of neural network architectures.\n",
    "- Embeddings map characters to continuous vectors of real numbers. The vectors are learned during training.\n",
    "- Each character in the string is represented by an embedding vector, and these vectors are combined to represent the entire molecular string.\n",
    "\n",
    "Fingerprint-Based Encoding:\n",
    "- Molecular fingerprints are binary vectors that encode the presence or absence of substructures (e.g., functional groups, atom types) in a molecule.\n",
    "- These fingerprints are used to represent molecular structures and can be fed directly into ANNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- each input should represent a conformation of a molecule\n",
    "- atomic environment vector as columns\n",
    "- Oliver's guess: >20 features for radial and >30 for spherical, ~50 features total.\n",
    "- radial corresponds to which shell it's on\n",
    "- spherical corresponds to orbital orientation\n",
    "- rotational and translational invariance: if conformation2 is generated by rotationor translationo of conformation1, both conformations should have the same energy and same input representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dr. Allgood suggests\n",
    "\n",
    "- RDkit\n",
    "- PCA to compact parameters\n",
    "- genetic algorithm to find optimal hyperparameters for deep learning\n",
    "    - you're gonna have to tune hyperparameters either way\n",
    "- can also use autoencoder (more pwerful PCA!)\n",
    "- for ANN, we can use the architecture in the paper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem277b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
